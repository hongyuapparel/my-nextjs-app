import { NextRequest, NextResponse } from 'next/server'

interface AIConfig {
  apiKey: string
  baseUrl: string
  model: string
  agentId?: string // Cherry Studioæ™ºèƒ½ä½“ID
  agentType?: 'cherry-studio' | 'chatgpt' | 'custom'
  gptId?: string // ChatGPT GPT ID
  openaiApiKey?: string // OpenAI API Key
}

interface RequestBody {
  message: string
  config: AIConfig
}

export async function POST(request: NextRequest) {
  try {
    const body: RequestBody = await request.json()
    const { message, config } = body

    // éªŒè¯å¿…è¦å‚æ•°
    if (!message) {
      return NextResponse.json(
        { error: 'ç¼ºå°‘å¿…è¦å‚æ•°ï¼šmessage' },
        { status: 400 }
      )
    }

<<<<<<< HEAD
    // ä»…ä½¿ç”¨å‰ç«¯æä¾›çš„å¯†é’¥ï¼Œä¸å†è¯»å–æœåŠ¡ç«¯ç¯å¢ƒå˜é‡
    const hasOpenai = !!(config.openaiApiKey && config.openaiApiKey.trim())
    const hasCherry = !!(config.apiKey && config.apiKey.trim() && config.agentId)

    // æ— å¯†é’¥æ—¶æä¾›ç¦»çº¿é™çº§æ–‡æœ¬å›å¤ï¼Œé¿å… 400
    if (config.agentType === 'chatgpt' && !hasOpenai) {
      const offline = `
ã€ç¦»çº¿æ¼”ç¤ºæ¨¡å¼ã€‘æœªé…ç½® OpenAI API Keyã€‚

ä½ æé—®çš„æ˜¯ï¼š\n${message}\n\nä¸‹é¢æ˜¯åŸºäºä¼ä¸šåŸ¹è®­åŠ©æ‰‹è§„èŒƒç»™å‡ºçš„é€šç”¨å»ºè®®ï¼ˆç¤ºä¾‹ï¼‰ï¼š\n- æ˜ç¡®ä½ çš„é—®é¢˜åœºæ™¯ä¸æœŸæœ›äº§å‡ºï¼ˆæ–‡æ¡£/è¡¨æ ¼/æ­¥éª¤ï¼‰\n- è‹¥æ¶‰åŠå…¬å¸å†…éƒ¨æµç¨‹ï¼Œè¯·é™„ä¸Šå½“å‰ SOP æ‘˜è¦æˆ–æˆªå›¾\n- å¦‚éœ€è‹±æ–‡å›å¤æ¨¡æ¿ï¼Œè¯·è¯´æ˜ç›®æ ‡å›½å®¶ä¸è¯­æ°”åå¥½\n\nåœ¨ç•Œé¢â€œé…ç½®â€ä¸­å¡«å†™ OpenAI Key åï¼Œå¯è·å¾—æ›´å‡†ç¡®çš„å›ç­”ã€‚`
      return NextResponse.json({
        response: offline,
        type: 'offline',
        model: 'offline',
        timestamp: new Date().toISOString(),
        success: true
      })
    }

    if (config.agentType === 'cherry-studio' && !hasCherry) {
      const offline = `
ã€ç¦»çº¿æ¼”ç¤ºæ¨¡å¼ã€‘æœªé…ç½® AiHubMix/Cherry Studio API Key æˆ– Agent IDã€‚

ä½ æé—®çš„æ˜¯ï¼š\n${message}\n\né€šç”¨ä¸šåŠ¡å»ºè®®ï¼ˆç¤ºä¾‹ï¼‰ï¼š\n1) å…ˆæ¾„æ¸…å®¢æˆ·æ ¸å¿ƒéœ€æ±‚ï¼ˆæ•°é‡ã€å°ºç ã€é¢æ–™ã€äº¤æœŸã€é¢„ç®—ï¼‰\n2) æä¾›å¯è½åœ°çš„ 1-2 å¥—è§£å†³æ–¹æ¡ˆï¼ˆå«é¢æ–™ä¸å·¥è‰ºï¼‰\n3) ç»™å‡ºè‹±æ–‡å›å¤èŒƒä¾‹ï¼Œå¹¶è¯´æ˜ä¸‹ä¸€æ­¥è¦å®¢æˆ·ç¡®è®¤çš„ç‚¹\n\nåœ¨ç•Œé¢â€œé…ç½®â€ä¸­å¡«å†™å¯†é’¥ä¸ Agent ID åï¼Œå¯è·å¾—æ›´ç²¾å‡†çš„æ™ºèƒ½ä½“å›å¤ã€‚`
      return NextResponse.json({
        response: offline,
        type: 'offline',
        model: 'offline',
        timestamp: new Date().toISOString(),
        success: true
      })
    }

    // æ ¹æ®æ™ºèƒ½ä½“ç±»å‹é€‰æ‹©è°ƒç”¨æ–¹å¼ï¼ˆæ­¤å¤„ä»…ä½¿ç”¨å‰ç«¯å¯†é’¥ï¼‰
    if (config.agentType === 'chatgpt' && config.gptId && hasOpenai) {
      const finalConfig = { ...config, openaiApiKey: (config.openaiApiKey || '').trim() }
      return await callChatGPTGPT(message, finalConfig)
    } else if (config.agentType === 'cherry-studio' && config.agentId && hasCherry) {
      const finalConfig = { ...config, apiKey: (config.apiKey || '').trim() }
      return await callCherryStudioAgent(message, finalConfig)
    } else {
      // å…œåº•ï¼šæ— æ•ˆé…ç½®æ—¶è¿”å›å‹å¥½æç¤º
      return NextResponse.json(
        { response: 'è¯·åœ¨ç•Œé¢â€œé…ç½®â€ä¸­æ­£ç¡®é€‰æ‹©æ™ºèƒ½ä½“å¹¶å¡«å†™æ‰€éœ€å¯†é’¥ã€‚', type: 'invalid-config' },
        { status: 200 }
=======
    // ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥ä½œä¸ºå¤‡ç”¨
    const envApiKey = process.env.AIHUBMIX_API_KEY
    const envOpenaiKey = process.env.OPENAI_API_KEY
    
    // æ ¹æ®æ™ºèƒ½ä½“ç±»å‹é€‰æ‹©è°ƒç”¨æ–¹å¼
    if (config.agentType === 'chatgpt' && config.gptId && (config.openaiApiKey || envOpenaiKey)) {
      const finalConfig = { ...config, openaiApiKey: config.openaiApiKey || envOpenaiKey || '' }
      return await callChatGPTGPT(message, finalConfig)
    } else if (config.agentType === 'cherry-studio' && config.agentId && (config.apiKey || envApiKey)) {
      const finalConfig = { ...config, apiKey: config.apiKey || envApiKey || '' }
      return await callCherryStudioAgent(message, finalConfig)
    } else {
      return NextResponse.json(
        { error: 'è¯·é…ç½®APIå¯†é’¥æˆ–ç¡®ä¿æ™ºèƒ½ä½“è®¾ç½®æ­£ç¡®' },
        { status: 400 }
>>>>>>> dd81c17ca42ee6716d780951d9d683820c388280
      )
    }

  } catch (error: any) {
    console.error('APIè°ƒç”¨é”™è¯¯:', error)
    
    return NextResponse.json(
      { 
        error: 'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯',
        details: error.message 
      },
      { status: 500 }
    )
  }
}

// è°ƒç”¨ChatGPT GPTs
async function callChatGPTGPT(message: string, config: AIConfig) {
  try {
    // é¦–å…ˆå°è¯•ä½¿ç”¨Assistant APIï¼ˆæ¨èæ–¹å¼ï¼‰
    if (config.gptId?.startsWith('asst_')) {
      return await callChatGPTAssistant(message, config)
    }
    
    // å¦‚æœæ˜¯GPT IDæ ¼å¼ï¼Œå°è¯•ç›´æ¥è°ƒç”¨
    if (config.gptId?.startsWith('g-')) {
      return await callChatGPTGPTDirect(message, config)
    }

    // å…œåº•ï¼šä½¿ç”¨æ™®é€šçš„GPT-4è°ƒç”¨ï¼Œä½†åŠ ä¸ŠGPTçš„æŒ‡ä»¤
    return await callChatGPTWithInstructions(message, config)

  } catch (error: any) {
    console.error('ChatGPTè°ƒç”¨é”™è¯¯:', error)
    return NextResponse.json(
      { error: 'ChatGPTæœåŠ¡è°ƒç”¨å¤±è´¥', details: error.message },
      { status: 500 }
    )
  }
}

// ä½¿ç”¨Assistant APIè°ƒç”¨ï¼ˆæ¨èï¼‰
async function callChatGPTAssistant(message: string, config: AIConfig) {
  try {
    // åˆ›å»ºçº¿ç¨‹
    const threadResponse = await fetch('https://api.openai.com/v1/threads', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${config.openaiApiKey}`,
        'OpenAI-Beta': 'assistants=v2'
      },
      body: JSON.stringify({})
    })

    if (!threadResponse.ok) {
      throw new Error(`åˆ›å»ºçº¿ç¨‹å¤±è´¥: ${threadResponse.status}`)
    }

    const thread = await threadResponse.json()

    // æ·»åŠ æ¶ˆæ¯åˆ°çº¿ç¨‹
    await fetch(`https://api.openai.com/v1/threads/${thread.id}/messages`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${config.openaiApiKey}`,
        'OpenAI-Beta': 'assistants=v2'
      },
      body: JSON.stringify({
        role: 'user',
        content: message
      })
    })

    // è¿è¡ŒåŠ©æ‰‹
    const runResponse = await fetch(`https://api.openai.com/v1/threads/${thread.id}/runs`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${config.openaiApiKey}`,
        'OpenAI-Beta': 'assistants=v2'
      },
      body: JSON.stringify({
        assistant_id: config.gptId
      })
    })

    if (!runResponse.ok) {
      throw new Error(`è¿è¡ŒåŠ©æ‰‹å¤±è´¥: ${runResponse.status}`)
    }

    const run = await runResponse.json()

    // è½®è¯¢è¿è¡ŒçŠ¶æ€
    let runStatus = run
    while (runStatus.status === 'in_progress' || runStatus.status === 'queued') {
      await new Promise(resolve => setTimeout(resolve, 1000))
      
      const statusResponse = await fetch(`https://api.openai.com/v1/threads/${thread.id}/runs/${run.id}`, {
        headers: {
          'Authorization': `Bearer ${config.openaiApiKey}`,
          'OpenAI-Beta': 'assistants=v2'
        }
      })
      
      runStatus = await statusResponse.json()
    }

    if (runStatus.status !== 'completed') {
      throw new Error(`è¿è¡Œå¤±è´¥: ${runStatus.status}`)
    }

    // è·å–å›å¤
    const messagesResponse = await fetch(`https://api.openai.com/v1/threads/${thread.id}/messages`, {
      headers: {
        'Authorization': `Bearer ${config.openaiApiKey}`,
        'OpenAI-Beta': 'assistants=v2'
      }
    })

    const messages = await messagesResponse.json()
    const aiMessage = messages.data[0]?.content[0]?.text?.value || 'æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚'

    return NextResponse.json({
      response: aiMessage,
      model: config.gptId,
      type: 'chatgpt-assistant',
      timestamp: new Date().toISOString()
    })

  } catch (error: any) {
    console.error('Assistant APIè°ƒç”¨é”™è¯¯:', error)
    throw error
  }
}

// ç›´æ¥è°ƒç”¨GPTï¼ˆå¯èƒ½ä¸æ”¯æŒç§æœ‰GPTï¼‰
async function callChatGPTGPTDirect(message: string, config: AIConfig) {
  try {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${config.openaiApiKey}`,
      },
      body: JSON.stringify({
        model: 'gpt-4', // ä½¿ç”¨æ ‡å‡†æ¨¡å‹
        messages: [
          {
            role: 'system',
            content: `ä½ ç°åœ¨è¦æ¨¡æ‹Ÿæˆ‘çš„GPTåŠ©æ‰‹çš„è¡Œä¸ºã€‚è¿™ä¸ªGPTçš„IDæ˜¯: ${config.gptId}ã€‚è¯·æŒ‰ç…§ä¼ä¸šåŸ¹è®­åŠ©æ‰‹çš„è§’è‰²æ¥å›ç­”é—®é¢˜ã€‚`
          },
          {
            role: 'user',
            content: message
          }
        ],
        temperature: 0.7,
        max_tokens: 1000,
        stream: false
      })
    })

    if (!response.ok) {
      throw new Error(`GPTç›´æ¥è°ƒç”¨å¤±è´¥: ${response.status}`)
    }

    const data = await response.json()
    const aiMessage = data.choices?.[0]?.message?.content || 'æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚'

    return NextResponse.json({
      response: aiMessage,
      model: 'gpt-4',
      type: 'chatgpt-gpt',
      gptId: config.gptId,
      timestamp: new Date().toISOString()
    })

  } catch (error: any) {
    console.error('GPTç›´æ¥è°ƒç”¨é”™è¯¯:', error)
    throw error
  }
    }

// ä½¿ç”¨æŒ‡ä»¤æ¨¡æ‹ŸGPTè¡Œä¸ºï¼ˆå…œåº•æ–¹æ¡ˆï¼‰
async function callChatGPTWithInstructions(message: string, config: AIConfig) {
  try {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${config.openaiApiKey}`,
      },
      body: JSON.stringify({
        model: 'gpt-4',
        messages: [
          {
            role: 'system',
            content: `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼ä¸šåŸ¹è®­åŠ©æ‰‹ï¼Œä¸“é—¨ä¸ºå‘˜å·¥æä¾›å…³äºå…¬å¸æ”¿ç­–ã€æµç¨‹ã€åŸ¹è®­ç­‰æ–¹é¢çš„å¸®åŠ©ã€‚è¯·ç”¨ä¸“ä¸šã€å‹å¥½çš„è¯­è°ƒå›ç­”é—®é¢˜ï¼Œæä¾›å…·ä½“ã€å¯æ“ä½œçš„å»ºè®®ã€‚å¦‚æœéœ€è¦å…·ä½“çš„å…¬å¸å†…éƒ¨ä¿¡æ¯ï¼Œè¯·å»ºè®®ç”¨æˆ·æŸ¥é˜…ç›¸å…³æ–‡æ¡£æˆ–è”ç³»HRéƒ¨é—¨ã€‚`
          },
          {
            role: 'user',
            content: message
          }
        ],
        temperature: 0.7,
        max_tokens: 1000,
        stream: false
      })
    })

    if (!response.ok) {
      throw new Error(`æŒ‡ä»¤æ¨¡æ‹Ÿè°ƒç”¨å¤±è´¥: ${response.status}`)
    }

    const data = await response.json()
    const aiMessage = data.choices?.[0]?.message?.content || 'æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚'

    return NextResponse.json({
      response: aiMessage,
      model: 'gpt-4',
      type: 'chatgpt-instructions',
      gptId: config.gptId,
      timestamp: new Date().toISOString()
    })

  } catch (error: any) {
    console.error('æŒ‡ä»¤æ¨¡æ‹Ÿè°ƒç”¨é”™è¯¯:', error)
    throw error
  }
}

// è°ƒç”¨Cherry Studioæ™ºèƒ½ä½“
async function callCherryStudioAgent(message: string, config: AIConfig) {
  try {
    console.log('ğŸ” è°ƒç”¨Cherry Studioæ™ºèƒ½ä½“:', config.agentId)
<<<<<<< HEAD

    // ç»Ÿä¸€æ¨¡å‹æ˜ å°„ + é™çº§åºåˆ—
    const requestedModel = (config.model || '').toLowerCase()
    const normalize = (m: string) => m.replace(/\s+/g, '').toLowerCase()
    const isVisionPreview = normalize(requestedModel).includes('vision')

    // Cherry/AiHubMix å¸¸è§å¯ç”¨æ¨¡å‹ï¼šgpt-4oã€gpt-4.1ã€gpt-4
    const fallbackModels = [
      isVisionPreview ? 'gpt-4o' : (requestedModel || 'gpt-4o'),
      'gpt-4.1',
      'gpt-4'
    ]

    const baseUrl = config.baseUrl || 'https://api.aihubmix.com/v1'
    const apiKey = (config.apiKey || '').trim()

    if (!apiKey) {
      const offline = `è¯·åœ¨â€œé…ç½®â€ä¸­å¡«å†™ AiHubMix API Key ä¸ Agent ID åå†è¯•ã€‚`
      return NextResponse.json({ response: offline, type: 'offline', model: 'offline' }, { status: 200 })
    }

=======
    
>>>>>>> dd81c17ca42ee6716d780951d9d683820c388280
    // ä½¿ç”¨ç”¨æˆ·åœ¨Cherry Studioä¸­è®¾ç½®çš„å®Œæ•´æç¤ºè¯
    const businessPrompt = `## æ‚¨çš„è§’è‰²ï¼šé¸¿å®‡æœé¥°çš„äº§å“å’Œä¸šåŠ¡é¡¾é—®

### æ‚¨çš„èŒè´£ï¼š
- **è¾“å…¥æ•°æ®ï¼š** å®¢æˆ·æä¾›çš„æœè£…éœ€æ±‚ï¼ŒåŒ…æ‹¬é¢æ–™é€‰æ‹©ã€æ¬¾å¼è®¾è®¡ã€åŠ å·¥å·¥è‰ºè¦æ±‚ç­‰ç»†èŠ‚ã€‚å…¥é—¨æ•°æ®å½¢æ€é€šå¸¸æ˜¯æ–‡å­—æè¿°æˆ–è®¾è®¡è‰å›¾ã€‚
- **è¾“å‡ºæ•°æ®ï¼š** ä¸šåŠ¡å»ºè®®ã€æœ€ä¼˜è®¾è®¡æ–¹æ¡ˆã€è‹±æ–‡å›å¤ä»¥åŠç”Ÿæˆçš„è®¾è®¡å›¾ç‰‡/å›¾ç‰‡æ•´åˆæ­¥éª¤ã€‚è¾“å‡ºæ•°æ®å½¢æ€æ˜¯æ–‡æœ¬æ–‡ä»¶å’Œå›¾åƒæ–‡ä»¶ã€‚
- **å·¥ä½œè¿‡ç¨‹ï¼š** 
  1. é˜…è¯»å®¢æˆ·éœ€æ±‚å’Œæä¾›çš„å›¾ç‰‡/è‰å›¾ã€‚
  2. è¯„ä¼°è¾“å…¥çš„åˆç†æ€§ï¼Œè€ƒè™‘é¢æ–™ç‰¹æ€§ã€å®¢æˆ·åœ°åŒºå·¥å‚åŠ å·¥æˆæœ¬ç­‰å› ç´ ã€‚
  3. æ ¹æ®é¸¿å®‡å…¬å¸çš„SOPæ–‡ä»¶ï¼Œæå‡ºåˆç†çš„è®¾è®¡æ–¹æ¡ˆå’Œä»·æ ¼è®¡ç®—ã€‚
  4. å¦‚è‹¥éœ€è¦ï¼Œä¸å…¬å¸è´Ÿè´£äººç£‹å•†å®Œå–„ä¿¡æ¯æˆ–è§£å†³å…·ä½“é—®é¢˜ã€‚
  5. åˆ›å»ºåˆæ ¼çš„ä¸šåŠ¡å›å¤ä»¥åŠå¯èƒ½éœ€è¦çš„è®¾è®¡å›¾ç‰‡/å›¾ç‰‡æ•´åˆã€‚

### æ‚¨çš„èƒŒæ™¯å’Œå—ä¼—ä¿¡æ¯ï¼š
- **èƒŒæ™¯ï¼š** å…·å¤‡æœè£…åŠ å·¥ã€é¢æ–™çŸ¥è¯†ã€æœ€ä¼˜è®¾è®¡ç†å¿µå’Œå…¬å¸å®é™…æ“ä½œè§„åˆ™èƒŒæ™¯ã€‚
- **å—ä¼—ï¼š** å…¬å¸çš„ä¸šåŠ¡å‘˜å’Œå®¢æˆ·ï¼Œéœ€æ±‚è§£ç­”åŠä½œä¸ºæœè£…äº§å“è®¾è®¡ä¸è®²è§£çš„å¯¹è±¡ã€‚

### æ‚¨çš„è¦æ±‚ï¼š
- é˜…è¯»å¹¶ç†è§£å®¢æˆ·æ”¶åˆ°çš„æœè£…éœ€æ±‚ã€‚
- åˆ†æéœ€æ±‚æ ¹æ®é¸¿å®‡å…¬å¸SOPå’Œè´¢åŠ¡åˆç†æ€§ï¼Œç»™å‡ºè§£å†³æ–¹æ¡ˆã€‚
- ç”¨å£è¯­åŒ–çš„è¯­æ°”ä¹¦å†™è‹±æ–‡è¯¦ç»†çš„å›å¤ï¼Œå¹¶æ ¹æ®éœ€è¦æä¾›è®¾è®¡å’Œå›¾ç‰‡æ•´åˆã€‚
- åœ¨ä¸ç¡®å®šæƒ…å†µä¸‹ï¼Œæå‡ºåŒ–è§£å»ºè®®å¹¶æé†’è´Ÿè´£äººæä¾›é¢å¤–è¾…åŠ©ã€‚
- ç¡®ä¿ä»»ä½•å›å¤å’Œå»ºè®®éƒ½èƒ½æå‡ä¸šåŠ¡å‘˜çš„æŠ€èƒ½å’Œå¯¹å®¢æˆ·æ»¡æ„åº¦çš„æå‡ã€‚

## Initialization
ä½œä¸ºé¸¿å®‡æœé¥°çš„äº§å“å’Œä¸šåŠ¡é¡¾é—®ï¼Œæ‚¨å°†æŒ‰ç…§ä»¥ä¸Šéƒ¨åˆ†è§„èŒƒå¼€å§‹å¤„ç†å’Œå›å¤å®¢æˆ·çš„æœè£…éœ€æ±‚ã€‚ç¡®ç«‹æµç¨‹å’Œæ¸…æ™°çš„å·¥ä½œè¦æ±‚ï¼Œå¹¶ä»¥å‹å¥½ä¸”ä¸“ä¸šçš„è¯­æ°”è¿›è¡Œäº¤æµï¼Œä¿ƒè¿›å…¬å¸ä¸šåŠ¡å’Œå®¢æˆ·æ»¡æ„åº¦çš„æå‡ã€‚`

<<<<<<< HEAD
    // æ„å»ºå…¬å…±ä½“
    const buildBody = (model: string) => {
      const body: any = {
        model,
        messages: [
          { role: 'system', content: businessPrompt },
          { role: 'user', content: message }
        ],
        temperature: 0.3,
        max_tokens: 2000,
        stream: false
      }
      if (config.agentId) {
        body.agent_id = config.agentId
        body.assistant_id = config.agentId
        body.knowledge_base = config.agentId
      }
      return body
    }

    let lastErrorText = ''
    for (const model of fallbackModels) {
      try {
        const reqBody = buildBody(model)
        console.log('ğŸ“¤ å‘é€è¯·æ±‚åˆ°:', `${baseUrl}/chat/completions`, 'model=', model)
        const response = await fetch(`${baseUrl}/chat/completions`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${apiKey}`,
            'X-Agent-ID': config.agentId || ''
          },
          body: JSON.stringify(reqBody)
        })

        if (response.ok) {
          const data = await response.json()
          const aiMessage = data.choices?.[0]?.message?.content || 'æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚'
          return NextResponse.json({ response: aiMessage, model, type: 'cherry-studio', agentId: config.agentId, timestamp: new Date().toISOString() })
        }

        const errorText = await response.text()
        lastErrorText = errorText
        console.warn('Cherry Studio APIé”™è¯¯:', response.status, errorText)

        // å¦‚æœæ˜¯æ¨¡å‹ä¸å¯ç”¨/æ— æƒé™ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
        if (/incorrect model id|model not found|no permission|ä¸å­˜åœ¨çš„æ¨¡å‹|æ— æƒé™/i.test(errorText)) {
          continue
        }

        // å…¶ä»–é”™è¯¯ç›´æ¥è¿”å›
        return NextResponse.json(
          { error: `Cherry Studio APIè°ƒç”¨å¤±è´¥ (${response.status}): ${errorText}` },
          { status: response.status }
        )
      } catch (err: any) {
        lastErrorText = err?.message || String(err)
        console.warn('è°ƒç”¨å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹:', model, lastErrorText)
        continue
      }
    }

    return NextResponse.json(
      { error: 'æ‰€æœ‰æ¨¡å‹å‡ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•æˆ–æ£€æŸ¥è´¦æˆ·æƒé™', details: lastErrorText },
      { status: 502 }
    )
=======
    // æ„å»ºè¯·æ±‚ä½“ï¼Œç¡®ä¿è°ƒç”¨æ‚¨çš„ä¸“å±æ™ºèƒ½ä½“
    const requestBody: any = {
      model: config.model || 'gpt-4',
      messages: [
        {
          role: 'system',
          content: businessPrompt
        },
        {
          role: 'user',
          content: message
        }
      ],
      temperature: 0.3, // é™ä½éšæœºæ€§ï¼Œä¿æŒä¸“ä¸šæ€§
      max_tokens: 2000, // å¢åŠ è¾“å‡ºé•¿åº¦
      stream: false
    }

    // æ·»åŠ Cherry Studioæ™ºèƒ½ä½“çš„ç‰¹å®šå‚æ•°
    if (config.agentId) {
      // ä½¿ç”¨æ­£ç¡®çš„å‚æ•°åè°ƒç”¨æ‚¨çš„æ™ºèƒ½ä½“
      requestBody.agent_id = config.agentId
      // ä¹Ÿå¯èƒ½éœ€è¦è¿™äº›å‚æ•°
      requestBody.assistant_id = config.agentId
      requestBody.knowledge_base = config.agentId
    }

    console.log('ğŸ“¤ å‘é€è¯·æ±‚åˆ°:', `${config.baseUrl}/chat/completions`)
    console.log('ğŸ“ è¯·æ±‚ä½“:', JSON.stringify(requestBody, null, 2))

    const response = await fetch(`${config.baseUrl}/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${config.apiKey}`,
        'X-Agent-ID': config.agentId || '', // é¢å¤–çš„headerå‚æ•°
      },
      body: JSON.stringify(requestBody)
    })

    if (!response.ok) {
      const errorText = await response.text()
      console.error('Cherry Studio APIé”™è¯¯:', response.status, errorText)
      
      return NextResponse.json(
        { error: `Cherry Studio APIè°ƒç”¨å¤±è´¥ (${response.status}): ${errorText}` },
        { status: response.status }
      )
    }

    const data = await response.json()
    const aiMessage = data.choices?.[0]?.message?.content || 'æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚'

    return NextResponse.json({
      response: aiMessage,
      model: config.model,
      type: 'cherry-studio',
      agentId: config.agentId,
      timestamp: new Date().toISOString()
    })
>>>>>>> dd81c17ca42ee6716d780951d9d683820c388280

  } catch (error: any) {
    console.error('Cherry Studioè°ƒç”¨é”™è¯¯:', error)
    return NextResponse.json(
      { error: 'Cherry StudioæœåŠ¡è°ƒç”¨å¤±è´¥', details: error.message },
      { status: 500 }
    )
  }
} 